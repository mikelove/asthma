---
title: "Exploratory and statistical analysis of RNA-seq data using DESeq2"
author: "Michael Love"
date: "June 2017"
abstract: "This workflow covers basic steps in an exploratory and statistical analysis of RNA-seq data, from import of Salmon quantifications, to performing differential gene expression and looking up annotation information about significant genes. The experimental data consists of control and virus treated human airway epithelial samples from 6 asthmatic and 6 non-asthmatic individuals, where a given individual has both a control (Vehicle) and treated (HRV, human rhinovirus) sample."
output:
  html_document:
    toc: true
    toc_float: true
---

```{r echo=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, fig.width=5, fig.height=5)
```

# Importing Salmon quant files

We begin this workflow by importing Salmon quantifications which are 
included in this repository under `data/quant`. Relative to the `scripts` 
directory, we can see we have quantifications for 24 samples:

```{r}
list.files("../data/quant/")
```

The layout of a single sample's quantification directory

```{r}
list.files("../data/quant/SRR1565926")
```

We will read in a table we created from the SRA website, which
gives some of the sample information. We call this table `coldata`,
because it provides data about the *columns* of the count matrix
we will be assembling.

```{r}
library(readr)
coldata <- read_delim("../data/SraRunTable.txt", delim="\t")
coldata
```

We have used the run ID (`SRR...`) to keep track of the reads and quantifications,
so we can build a vector which points to our quantification files using
this column of `coldata`. We use `names` to name this vector with the run IDs as well.

```{r}
files <- file.path("../data/quant",coldata$Run_s,"quant.sf.gz")
names(files) <- coldata$Run_s
head(files,2)
```

The following code (not evaluated here) can be used to generate a table
that connects transcripts to genes for summarizing Salmon transcript
quantifications for differential gene expression. We simply
read in the GTF file from the same database that we used for building
the Salmon index (in this case, Gencode version 26), and then pull
out a table with the transcript name for every gene.

```{r eval=FALSE}
# ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_26/gencode.v26.annotation.gtf.gz
library(GenomicFeatures)
txdb <- makeTxDbFromGFF("gencode.v26.annotation.gtf.gz")
saveDb(txdb, file="gencode.v26.sqlite")
# next time you can just load with this line (no need to makeTxDb...)
# txdb <- loadDb("gencode.v26.sqlite") 
columns(txdb)
k <- keys(txdb, "GENEID")
res <- AnnotationDbi::select(txdb, k, "TXNAME", "GENEID")
tx2gene <- res[,2:1]
```
	
We have prepared this table in advance, and now load it:
	
```{r}
load("../data/tx2gene.rda")
head(tx2gene)
```

Now we can use the `tximport` function to assemble all the quantifications
from the 24 files, and to summarize the abundances, counts and transcript
lengths to the gene level, for use with DESeq2 and other Bioconductor
packages.

It's a good idea to first test on a single quantification file, which we show here:

```{r}
library(rjson)
library(tximport)
txi <- tximport(files[1], type="salmon", tx2gene=tx2gene)
```

Now we can run `tximport` over all the quanfitication files.
We can see we obtain a list of matrices with common dimension:
58219 (the number of genes) x 24 (the number of samples).

```{r message=FALSE}
txi <- tximport(files, type="salmon", tx2gene=tx2gene)
names(txi)
dim(txi$abundance)
dim(txi$counts)
dim(txi$length)
```

Now we load DESeq2 for further steps in the workflow:

```{r message=FALSE}
library(DESeq2)
```

# Assembling the sample info

In the `coldata` table, we have information about which samples are from
asthmatic or non-asthmatic individuals, and which samples are control or treated.
Because we built `txi` using the run IDs, we know that these columns are
lined up with our columns of the matrices in `txi`.

```{r}
coldata$disease_state_s
coldata$treatment_s
```

While most of the information we need is in the `coldata` table already,
while preparing this data for analysis, I noticed that the same subjects had 
both a control (Vehicle) and treated (HRV16) sample, but I didn't find this
information from the SRA table. It was present, however, in the title of 
the samples listed on the GEO website, which also points to the run ID.
We can therefore bring in the sample names from GEO, line them up with
our coldata, and extract the subject ID information:

```{r}
geo <- read_delim("../data/GEO_table.txt", delim="\t", col_names=FALSE)
head(geo)
coldata$title <- geo$X2[match(coldata$Sample_Name_s, geo$X1)]
coldata$condition <- factor(coldata$disease_state_s)
coldata$treatment <- factor(coldata$treatment_s)
```

Now, we will build a `DESeqDataSet` from the matrices in `txi`, 
which we will use for the rest of the workflow. This function brings
along the estimated counts per gene, estimated by Salmon, as well as 
a normalizing offset based on the transcript lengths. This normalizing offset
adjusts for the *average transcript length* of a gene, which can be influenced
by differential isoform usage, as well as common RNA-seq biases,
if we used Salmon flags for correcting for various biases. Both of these effects 
-- differential isoform usage and technical biases -- 
can change the *effective length* of a gene, and so both are useful as
normalizing offsets in a statistical comparisons of counts across samples.

When building the `DESeqDataSet` we have to specify a *design*, which
is a formula in R that begins with a tilde and explains what terms, or coefficients,
we want to use to model the counts. The design is used by the dispersion estimation
and model fitting functions in DESeq2, so we can change it later, but we will have 
to rerun the main functions to re-estimate the parameters. 

For now, we will use a design that specifies a condition effect (asthmatics vs
non-asthmatics), a treatment effect (HRV16 vs Vehicle), and an interaction between 
the two (so the treatment effect can be different for asthmatics and non-asthmatics).
An interaction term is specified in R with a colon between two variables.
This design roughly corresponds to the goals of the original study.
The samples are human airway epithelial cells, and so we can expect to see a reaction
in these cells upon treatment with virus.

```{r}
dds <- DESeqDataSetFromTximport(txi, coldata,
                                ~condition + treatment + condition:treatment)
dds
```

I like to rename the *levels* of the variables in the design so they are
easier to work with, by shortening them.

```{r}
# you can rename levels, but need to use same order as current levels()
levels(dds$condition)
levels(dds$condition) <- c("asth","non")
levels(dds$condition)
dds$condition
```

It's also important to set the *reference level* in a sensible way,
so comparisons are of treated over control for instance. In this 
case the reference levels should be the non-asthmatic individuals and the
Vehicle treatment.

We use the compound assignment operator `%<>%` from the magrittr package, 
which saves us a little extra typing, when we want to apply a function 
to a variable in R, and then re-assign it (so it is equivalent to `x <- f(x)`).

```{r}
library(magrittr)
dds$condition %<>% relevel("non")
dds$treatment %<>% relevel("Vehicle")
dds$condition
dds$treatment
```

# Exploratory data analysis

Already, we can take a look at how the samples related to each other.
In DESeq2, we have special functions for transforming the counts,
so that they can be easily visualized (we will not transform the counts, 
but use the raw counts later, for statistical testing).

My favorite of these transformation is the `vst`, mostly because it is 
very fast, and provides transformed (nearly log-scale) data which is
robust to many problems associated with log-transformed data (for more details,
see the DESeq2 
[workflow ](http://www.bioconductor.org/help/workflows/rnaseqGene/#the-rlog-and-variance-stabilizing-transformations)
or 
[vignette](https://www.bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#count-data-transformations)
).

`blind=FALSE` refers to the fact that we will use the *design* in estimating
the global scale of biological variability, but not directly in the transformation:

```{r}
vsd <- vst(dds, blind=FALSE)
```

Now that we have normalized and transformed the data, it will have roughly 
the same variance (except for differentially expressed genes) across the range of
counts, so from counts in the single digits, up to the most highly expressed 
genes with very high counts.

We can make a PCA plot, which shows the distribution of the samples
among the top two dimensions, in terms of the variance explained.
It's simply a rotation and projection of the transformed data, but 
picking the "best" 2 dimensions out of the tens of thousands (number of genes).

```{r pca, fig.width=7, fig.height=4}
plotPCA(vsd, c("treatment","condition"))
```

From the PCA plot, we see that the treatment with HRV leads to the most
variance across samples, for the top variable genes. There seems to be some
clustering by disease status (what we called `condition`), for the treated samples
but not much for the control samples.

# Re-arrange sample info

As we mentioned before, there is an additional piece of information
about the samples: the Vehicle and HRV treated samples are from
the same individual, so this is also important information to include
in the design, if possible. In this case, because we are comparing
control and HRV treatment within individuals, we can add this information 
to the design. First, we need to clean up the sample ID information
contained in the `title` variable:

```{r}
dds$id <- substr(dds$title, 1, 3)
dds$id
id.lvls <- c(dds$id[dds$condition == "non" & dds$treatment == "Vehicle"],
             dds$id[dds$condition == "asth" & dds$treatment == "Vehicle"])
id.lvls
```

We will re-factor the id, so that the levels are in the order
of the `id.lvls` variable we just defined.

We will then re-order the `DESeqDataSet` so that the
samples are in order by condition, treatment and ID.

```{r}
dds$id %<>% factor(levels=id.lvls)
o <- order(dds$condition, dds$treatment, dds$id)
dds <- dds[,o]
```

We can take a look at the `colData` to confirm it's in the order
as we want it to be:

```{r}
as.data.frame(colData(dds)[c("condition","treatment","id")])
all(dds$id == c(rep(id.lvls[1:6], 2),
                rep(id.lvls[7:12], 2)))
```

To make the within-individual treatment comparisons across 
condition, we need to do a little re-coding trick for the 
subject ID. We will re-code them so that the first asthmatic 
subject is called `1`, and the first non-asthmatic subject 
is also called `1`, which we call "nesting". 

Note that these two subjects will 
not be treated as the same in the model, because we will
include an interaction term between `condition` and `id.nested`.

```{r}
dds$id.nested <- factor(rep(1:6,4))
as.data.frame(colData(dds)[c("condition","treatment","id","id.nested")])
```

Now we update the design, so that each patient gets his or her 
own reference level for comparison of the treatment effect:

```{r}
design(dds) <- ~condition + condition:id.nested +
  treatment + condition:treatment
```

Before we run the differential expression steps,
we have one more data cleaning step to do. We will 
chop off the version number of the gene IDs, so that we 
can better look up their annotation information later.

However, we have a few genes which would have duplicated
gene IDs after chopping off the version number, so in order
to proceed we have to also use `make.unique` to indicate 
that some genes are duplicated. (It might be 
worth looking into why we have multiple versions of genes
with the same base ID coming from our annotation.)

```{r}
head(rownames(dds))
table(duplicated(substr(rownames(dds),1,15)))
rownames(dds) <- make.unique(substr(rownames(dds),1,15))
```

# Differential gene expression

Now we can run our differential expression pipeline.
First, it is sometimes convenient to remove genes where
all the samples have very small counts. It's less of an issue 
for the statistical methods, and mostly just wasted computation,
as it is not possible for these genes to exhibit statistical
significance for differential expression. Here we count
how many genes (out of those with at least a single count)
have 3 samples with a count of 10 or more:

```{r}
dds <- dds[rowSums(counts(dds)) > 0,]
keep <- rowSums(counts(dds) >= 10) >= 3
table(keep)
dds <- dds[keep,] # filter them out
```

Now we can run the differential expression pipeline using
`DESeq` and extract the results using `results`.
These functions do a little of work for you, and
they have extensive help describing all their options,
which can be read by typing in `?DESeq` and `?results`.

We will build a results table for the coefficient
`conditionasth.treatmentHRV16`. This coefficient represents
the difference in the treatment effect in the asthmatic group
relative to the non-asthmatic group.

```{r}
dds <- DESeq(dds)
resultsNames(dds)
res <- results(dds, name="conditionasth.treatmentHRV16")
res.sort <- res[order(res$pvalue),]
```

# Exploring results

A good visual summary of a results table is the "MA-plot".
M stands for "minus", as the y-axis for a simple two group
comparison is the difference between the log of the expression
values for each group. In general, and for this experiment, the y-axis
is the log2 fold change attributable to the coefficient or contrast
that was used in building the results table. The "A" stands for average,
as the x-axis indicates the average of normalized counts across 
all the samples in the dataset.

Because all of the points are grey, we know that none of the 
genes showed a significant difference in the treatment effect
across the two condition groups, at an FDR cutoff of 0.1 
(this is the default value for `plotMA`, and can be changed).

```{r plotma}
plotMA(res, ylim=c(-5,5))
```

We can also print out a summary table, which 
similarly tells us that, at an FDR cutoff of 0.1,
no genes were significantly differentially expressed
for our particular comparison.

```{r}
summary(res)
```

```{r echo=FALSE}
# to make plotCounts same each time
# (has random jitter)
# avoids inflation of git repo...
set.seed(1)
```

While we didn't get any genes at an FDR cutoff of 0.1, we can
look at the top gene by adjusted p-value, in 
terms of the normalized counts in the different groups.

There does seem to be a trend of downregulation of this gene
for non-asthmatics, and up-regulation for asthmatics, 
but generally the fold changes across treatment are not very
consistent within conditions.

We've added the ID within each condition as a plotting character
`pch`:

```{r topgene1, fig.width=7, fig.height=5}
top.gene <- rownames(res.sort)[1]
plotCounts(dds, top.gene, c("condition","treatment"), 
           transform=FALSE, pch=as.integer(dds$id.nested))
```

We can also make a plot which draws lines
between the expression values across treatment for a given sample.
To do so, we need to use the `ggplot2` library. First, we 
export a little table of the counts and design variables 
for the top gene:

```{r}
dat <- plotCounts(dds, top.gene, c("condition","treatment","id.nested"),
                  returnData=TRUE)
```

Next we form the `ggplot2` code, using points and a smooth line
to connect the points for each ID in each condition group.
It makes sense that this is the top gene for testing different slope
across condition, but the slopes are not entirely consistent
across the samples within a condition, which explains why
it's not more significant in the hypothesis test.

```{r targets2, warning=FALSE, fig.width=7, fig.height=4}
library(ggplot2)
ggplot(dat, aes(x=treatment, y=count, col=id.nested, group=id.nested)) +
  geom_point() + geom_smooth(method="lm", se=FALSE) +
  scale_y_log10() + 
  facet_wrap(~condition)
```

We can look up the gene symbol for the top gene using an annotation package.
These packages have a number of functions for pulling out annotations,
here we will show the `mapIds` function and the `select` function.
`select` is the name for a function in the `dplyr` package,
so we have to use the package prefix `AnnotationDbi::` to call
our version of `select`.

The other command is for looking up gene ontology terms for the top gene, 
specifically terms that are classified as biological processes (BP).
We will explore GO terms further in a later section of this workflow.

```{r}
library(org.Hs.eg.db)
org.Hs.eg.db %>% mapIds(top.gene, "SYMBOL", "ENSEMBL")
go.tab <- org.Hs.eg.db %>% AnnotationDbi::select(top.gene, "GO", "ENSEMBL") %>% subset(ONTOLOGY == "BP")
go.tab
```

A number of gene symbols were listed in the abstract of the paper 
(one of which we swapped here for a more common gene symbol).
We can do a reverse lookup, to see where they are showing up
in our list of ranked genes:

```{r}
target <- c("CCL5","CXCL10","CX3CL1","ACKR4","CDHR3")
target.map <- mapIds(org.Hs.eg.db, target, "ENSEMBL", "SYMBOL")
target.map
match(target.map, rownames(res.sort))
```

Let's take a look at the counts for the second gene symbol from above:

```{r targets, fig.width=7, fig.height=5}
plotCounts(dds, target.map[2], c("condition","treatment"))
plotCounts(dds, target.map[2], c("condition","treatment"), transform=FALSE)
```

# Interlude: power analysis

With your standard t-test or linear regression on a one dimensional *Y*, 
performing a post-hoc power analysis doesn't make sense, because the observed effect
size and variance estimate functionally determine the estimated power just as 
they do the observed p-value. In other words, it's entirely 
circular to show that with a standard power analysis formula,
using the post-hoc observed effect size and variance, it will say 
the experiment was "under-powered", if you have an observed 
p-value above your critical threshold (and the converse). 
It makes more sense to perform power analyses for a range of effect sizes, 
and using your best estimate of within-group variance.

Here we will do something a little different, we will assess via 
simulation what range of effect sizes and count ranges would have
been significant, given the observed variance of counts.
We will work with a value called *dispersion* which links
the variance to the mean in a negative binomial distribution 
(this is the distribution used to model counts in DESeq2).
Roughly, the dispersion is the square of the coefficient of
variation of counts. So dipersion of 0.01 implies that the counts
might vary by about 10% around their mean value.

```{r}
sqrt(.01)
```

DESeq2 estimates dispersion values for each gene, first a 
*maximum likelihood value*, and then a *maximum posterior* value.
For full details, you can take a look at the 
[DESeq2 paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8), 
but briefly, the maximum likelihood value just looks at the data for a single
gene, while the maximum posterior value integrates the information
from the individual gene with the information about all the genes.
If the maximum likelihood estimate for a single gene 
differs greatly from the estimates of the majority of the other genes,
that gene's estimate will be "pulled in" or "shrunk" towards the estimates 
of the others. It's not shrunk all the way, and the amount is determined
by a formula called *Bayes formula*. It can be shown that these posterior
estimates have less error overall than the maximum likelihood estimates,
if we structure the formula in the right way. They end up preventing
a lot of false positives, where we might by chance have a 
too-small dispersion estimate for some genes, in particular when 
the sample sizes are small.

Here we can plot the dispersion estimates for this dataset.
The black points are the maximum likelihood estimates ("gene-est"),
and the blue points are the maximum posterior estimates ("final").

```{r plotdisp}
plotDispEsts(dds, ylim=c(1e-4,10))
```

We can extract information about how the trend and how the values 
clustered around the trend. We will write an R function
that takes in a mean normalized count, and outputs
a plausible dispersion value for this dataset.
The `.05` and `2.06` come from the coefficients for the red line,
and the `.28` comes from determining how far the true dispersion values
are spread around the red line. It's less than the black points,
as the spread of the black points represents dispersion differences 
across genes *and* sampling variance.

The `.05` *asymptotic dispersion* means that the counts
vary by about 22% of their expected value, when the counts are high.

```{r}
dispersionFunction(dds)
dmr <- function(x) (.05 + 2.06 / x) * exp(rnorm(length(x),0,.28))
```

This is what our `dmr` function returns, when we
feed in the mean of normalized counts from the dataset.

```{r simdisp}
baseMean <- mcols(dds)$baseMean
plot(baseMean, dmr(baseMean), log="xy", ylim=c(1e-4,10))
```

We can see what the mean and standard deviation
of the log2 of mean counts was. Note that this
experiment had much lower sequencing depth than 
is typical for a human RNA-seq dataset. Typically, 
total counts of 20-40 million are common. This
dataset has about 1/10 of that. The mean count is about
`2^5` or 32.

```{r}
mean(log2(baseMean))
sd(log2(baseMean))
```

There is a function in DESeq2 to simulate a dataset
according to parameters estimated from another dataset.
Note that, while the previous experiment had a complex
design structure including control for subject and condition,
here we just simulate a two group analysis for
simplicity. We simulate 10,000 genes with 12 samples across two groups,
and supposing that the true log2 fold changes are centered at 
0 with a standard deviation of 1. This means that fold changes
of 1/2 or 2 are common. This will allow us to see how high
the fold changes will need to be to end up in the set of
differentially expressed genes.

```{r}
set.seed(1)
sim <- makeExampleDESeqDataSet(n=10000, m=12,
                               betaSD=1,
                               interceptMean=5,
                               interceptSD=2,
                               dispMeanRel=dmr)
keep <- rowSums(counts(sim) >= 10) >= 3
table(keep)
sim <- sim[keep,]
sim <- DESeq(sim)
sim.res <- results(sim, independentFiltering=FALSE, cooksCutoff=FALSE)
```

We can plot the MA plot from our data, with the simulated data 
side-by-side. With the caveat that the experimental designs are 
not the same, in the simulated data we can see that large fold
changes are detected as differentially expressed at 10% FDR,
and that this depends on the mean count value. For the smallest 
mean count genes, the fold change needs to be larger for
the gene to end up in the significant set.

```{r compma, fig.width=8, fig.height=5}
par(mfrow=c(1,2))
plotMA(res, xlim=c(1,1e6), ylim=c(-5,5))
plotMA(sim.res, xlim=c(1,1e6), ylim=c(-5,5))
```

We can use `dplyr` and `ggplot2` to perform a more in depth
comparison of the power at various levels of effect size (log2 fold change)
and mean count. Because we simulated the data, we can use
the true log2 fold change and true mean value for the "control"
group, in order to classify the genes into bins.

```{r}
max.lfc <- ceiling(max(abs(mcols(sim)$trueBeta)))
sim.dat <- data.frame(sig=sim.res$padj < .1,
                      mean=cut(2^mcols(sim)$trueIntercept,c(0,10,100,1000,1e5)),
                      abs.LFC=cut(abs(mcols(sim)$trueBeta),c(0,.25,.5,1,max.lfc)))
```

We group the genes by the mean value and absolute fold change, and calculate the
statistical power in terms of number of times the adjusted p-values 
are less than 0.1 (and the genes would end up in the significant set).

```{r message=FALSE}
library(dplyr) 
sim.tab <- sim.dat %>% group_by(mean, abs.LFC) %>% summarize(power=mean(sig))
```

Plotting these power curves, we can see that the power is low for genes 
with small absolute LFC and genes with small mean count. To achieve
75% power for detection, the log2 fold change for a gene with mean count
in the range 10-100 needs to be greater than 1.

```{r power, fig.width=7, fig.height=4}
ggplot(sim.tab, aes(x=abs.LFC, y=power, col=mean, group=mean)) + geom_line()
```

While we have our dispersion-mean function, we can see how DESeq2
performs when many of the genes are *null*, that is the log2 fold
change is exactly equal to 0. We simulate another dataset,
with 80% of null genes and a wider spread of mean counts.

```{r}
de <- rep(c(FALSE,TRUE),c(8000,2000))
set.seed(1)
sim2 <- makeExampleDESeqDataSet(n=10000, m=12,
                                betaSD=ifelse(de,1,0),
                                interceptMean=6,
                                interceptSD=3,
                                dispMeanRel=dmr)
sim2 <- DESeq(sim2)
```

We can then see how many null genes end up in FDR sets at
increasing cutoffs.

```{r}
threshold <- c(1,5,10,15,20)/100
FDR <- sapply(threshold, function(t) {
  sim2.res <- results(sim2, alpha=t)
  sig <- which(sim2.res$padj < t)
  mean(!de[sig])
})
```

The rate of null genes is roughly on target with the 
*nominal* amount, that is, the amount of false discovery
rate we asked for.

```{r simfdr}
plot(threshold, FDR, ylim=c(0,.3), type="b", col="blue",
     main="Sim: empirical vs nominal FDR")
abline(0,1)
```

We can also do a quick power and precision analysis of 
DESeq2 calls using the highly replicated yeast RNA-seq dataset of 
[Schurch et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4878611/).
This dataset, designed for benchmarking RNA-seq for power analysis,
has more than 40 biological replicates in two condition, one of 
wild type yeast and a mutant strain, *Î”snf2*.

```{r}
load("../data/yeast.rda")
dim(yeast)
table(yeast$condition)
```

We can compare an analysis with 5 samples randomly drawn from each group,
with the results from running an analysis on the held-out samples. We would
expect that with 37-39 samples in the held-out set, we are close to finding 
all the differentially expressed genes, and so this can be considered a 
"gold standard" set.

```{r}
n <- 5
set.seed(1)
idx <- c(sample(which(yeast$condition == "WT"), n),
         sample(which(yeast$condition == "mut"), n))
yeast.in <- yeast[,idx]
yeast.out <- yeast[,-idx]
yeast.in <- DESeq(yeast.in)
yeast.out <- DESeq(yeast.out)
```

The mutation in this experiment results in the majority of genes having small, 
statistically significant fold changes, so it's a bit too easy 
if we would look to see which genes are common across the two
analyses. We will require that the log2 fold changes show statistical
evidence of being larger than 1 in absolute value. And we will
require that a low FDR cutoff in the held-out set, 1%, because we 
think that we have sufficient replicates (37-39), to have sufficient power
to detect these genes.

```{r}
res.in <- results(yeast.in, lfcThreshold=1, alpha=.1)
res.out <- results(yeast.out, lfcThreshold=1, alpha=.01)
res.out$padj[is.na(res.out$padj)] <- 1 # set NA to 1
de <- res.out$padj < .01 # consider 'out' with 1% FDR as 'truth'
prop.table(table(test=res.in$padj < .1, de),1)
```

We can look to see how many genes in the subset are called at various
FDR thresholds, and then we calculate the number of "null" genes,
that is, genes which do not show statistical evidence of having an LFC
larger than 1 in absolute value in the held-out set:

```{r}
sapply(threshold, function(t) {
  res.in <- results(yeast.in, lfcThreshold=1, alpha=t)
  sum(res.in$padj < t, na.rm=TRUE)
})
FDR <- sapply(threshold, function(t) {
  res.in <- results(yeast.in, lfcThreshold=1, alpha=t)
  sig <- which(res.in$padj < t)
  mean(!de[sig])
})
```

From this analysis, DESeq2 is close to the target for
low cutoffs, but then as we build larger sets with higher
cutoff values, we have lower FDR than nominal for the test of 
LFC greater than 1.

```{r yeastfdr}
plot(threshold, FDR, ylim=c(0,.3), type="b", col="blue",
     main="High replicate yeast: empirical vs nominal FDR")
abline(0,1)
```

# Other differential analyses

Now that we've taken a look at how power depends on effect size and mean count 
(among other things like design, number of replicates and dispersion),
let's return to our dataset, and try different statistical analyses,
besides the test of differences across condition in the treatment effect.

We didn't seem to see much of a difference in the treatment effect
across condition, so we can try another design, in which we estimate
the same treatment effect in both conditions, comparing within
subjects.

```{r}
dds2 <- removeResults(dds)
design(dds2) <- ~condition + treatment + condition:id.nested
dds2 <- DESeq(dds2)
resultsNames(dds2)
res2 <- results(dds2, name="treatment_HRV16_vs_Vehicle")
```

The above results table is equivalent, produced with the `name` argument
is equivalent to using the `contrast` argument, and providing
the numerator and denominator for the contrast:

```{r}
res2 <- results(dds2, contrast=c("treatment","HRV16","Vehicle"))
```

We can again make an MA plot, and notice that there are now
many genes which show large and significant log2 fold changes.
Also, one can see that most of the genes with log2 fold change 
larger than 2 in absolute value are in the top, meaning
that we are seeing genes with large up-regulation upon HRV treatment.

```{r plotma2}
plotMA(res2, ylim=c(-10,10))
```

```{r}
summary(res2)
```

We will take a look at the genes with large, positive log2 fold change
(greater than 2), and sort by the log2 fold change.

Looking at the gene names, some of the symbols look familiar, 
e.g. the ones with `CXCL...` and `CCL5`. These genes code for chemokines,
which are signaling molecules in the cell, and it makes sense to see these
up-regulated after treatment with virus, as the cells are mounting an
immune defense.

```{r}
res2.up <- results(dds2, name="treatment_HRV16_vs_Vehicle", 
                   lfcThreshold=1, altHypothesis="greater")
res2.up <- res2.up[res2.up$padj < .1,]
res2.sort <- res2.up[order(res2.up$log2FoldChange, decreasing=TRUE),]
org.Hs.eg.db %>% mapIds(rownames(res2.sort)[1:40],
                        "SYMBOL", "ENSEMBL")
```

Note that some of the top genes from the abstract are high on this list
of genes differentially expressed upon viral treatment.

```{r}
match(target.map, rownames(res2.sort))
```

# Exploring results with annotation

We can dive deeper into the top genes, by looking up
what biological processes these are associated with.

```{r}
go.tab <- org.Hs.eg.db %>% AnnotationDbi::select(rownames(res2.sort)[1],
                                  "GO", "ENSEMBL") %>% subset(ONTOLOGY == "BP")
go.tab
```

Now that we have associated this gene with a set of GO terms, we can look up
their names. Sometimes the names are very long, so to fit on the screen
we will chop the name at 60 characters.

The biological processes have names like "inflamation response", "immune response",
"response to cold", and "defense response to virus", which make sense.

A statistical rigorous approach, if we didn't have any terms in mind, would
be to perform a *battery* of statistical tests of all GO biological process 
terms. Sometimes, you might consider all GO terms within a range by size, 
e.g. GO BP terms with >= 10 and <= 500 genes.

Some packages and functions for performing gene set testing are 
[goseq](http://bioconductor.org/packages/goseq),
*roast*, and *camera*, the last two which are in the 
[limma](http://bioconductor.org/packages/limma) 
package.

```{r}
library(GO.db)
go.tab2 <- GO.db %>% AnnotationDbi::select(go.tab$GO, "TERM", "GOID")
substr(go.tab2$TERM, 1, 60)
```

We can write a function which prints out the GO term names for a given gene in our
results table:

```{r}
getTerms <- function(n) {
  go.tab <- org.Hs.eg.db %>% AnnotationDbi::select(rownames(res2.sort)[n],
              "GO", "ENSEMBL") %>% subset(ONTOLOGY == "BP")
  go.tab2 <- GO.db %>% AnnotationDbi::select(go.tab$GO, "TERM", "GOID")
  substr(go.tab2$TERM, 1, 60)
}
```

We see a lot of these immune response terms, but again
the proper way to do this would be (1) to have a specific
process in mind (no peeking at these lists first),
or (2) to test against a battery of GO-defined gene sets
of a certain size.

```{r}
getTerms(2)
getTerms(3)
getTerms(4)
getTerms(5)
getTerms(6)
```

# LFC shrinkage

Finally, I want to show a feature of DESeq2: the 
Bayesian shrinkage of log2 fold changes. The utility
of *shrunken* log2 fold changes is that, it is sometimes 
useful to do further data analysis or visualization 
of effect sizes, rather than converting these into Wald
statistics (dividing by their standard error) or discretizing
into FDR-bounded sets of differentially expressed genes.
Some examples of such analyses are if we wanted to plot
log2 fold change across experiments, or if we wanted to cluster
groups of samples by log2 fold change. Applying a Bayesian
procedure to the log2 fold changes has a number of advantages
relative to the use of pseudocounts in moderating the extreme ratios
one can obtain when counts are low or highly variable.

The log2 fold change shrinkage feature 
currently only works when the design does not contain 
interaction terms, as we did not yet find a suitable method
for dealing with interaction and main effects and to produce
the effect size shrinkage. We're still working on a method
in this area, but for now, we can use a design with just
the condition and treatment terms (so ignoring the terms
accounting for each subject).

```{r}
dds3 <- removeResults(dds)
design(dds3) <- ~condition + treatment
```

We re-run the pipeline, and now we use a function `lfcShrink`,
to shrink the log2 fold change due to treatment, which 
is the 3rd coefficient in `resultsNames`.

```{r}
dds3 <- DESeq(dds3)
resultsNames(dds3)
res3 <- results(dds3, lfcThreshold=2)
res3shr <- lfcShrink(dds3, coef=3, res=res3)
```

We can compare the shrunken log2 fold changes to the maximum
likelihood log2 fold changes. We color in the genes with a normalized
count of only 12 summing across the control samples. Here we have
an average of just one count per sample. 

On the right side, we can see that these genes have a reduction in their
estimated fold change, where now the genes with higher mean normalized
count, and therefore more statitistical information underlying
the observed ratio, are given a bit more priority in the ranking by log2 fold change.
Two of the genes which had a high log2 fold change previously are now near 0,
indicating that there was very little information to justify the large log2 fold change.

```{r plotma3, fig.width=8, fig.height=5}
par(mfrow=c(1,2))
plotMA(res3, ylim=c(-12,12))
rs <- rowSums(counts(dds, normalized=TRUE)[,dds$treatment == "Vehicle"])
too.low <- rs < 12
with(res3[too.low,], points(baseMean, log2FoldChange, cex=2, col="dodgerblue"))
plotMA(res3shr, ylim=c(-12,12))
with(res3shr[too.low,], points(baseMean, log2FoldChange, cex=2, col="dodgerblue"))
```

# Session info

```{r}
sessionInfo()
```
